멀티쓰레드 프로그래밍 가이드
- Homogeneous 멀티쓰레드 프로그래밍
- Locking의 회피
- 공유 자료 구조의 사용

메모리 일관성
지금까지의 프로그램은 공유 메모리에 대한 접근(쓰기/읽기)은 atomic하다고 가정하고 있다.
- 정말 그런가?

atomic
- 메모리의 접근이 순간적으로 행해지며, 서로 겹쳐지지 않는다.
- 실행결과 순서가 정해지면, 모든 쓰레드에서 같은 순서로 보인다.

- PC에서의 메모리 접근은 atomic이 아니다.
- 메모리에 쓴 순서대로 메모리가 수정되지 않는다.
	. 정확히는 메모리에 쓴 순서대로 메모리의 내용이 관측되지 않는다.

- 피터슨 알고리즘에서 역전
	. Write가 되기 전에 Read를 한다.
	. _asm mfence 명령어를 통해 실행 순서를 강제로 맞춰 줄 수 있다.

메모리 접근이 atomic하지 않은 이유는?
- CPU는 사기를 친다
	. Line Based Cache Sharing
	. Out of Order Execution (비순차적 명령어 처리)
	. Write Buffering
	. 사기를 치지 않으면 실행속도가 느려진다.
- CPU는 프로그램을 순차적으로 실행하는 척만 한다.
	. 이를 보조해 주는 특수한 HW가 존재한다.
	. 싱글코어에서는 절대로 들키지 않는다.

Out of Order Execution
- CPU는 관계가 없는 명령어끼리는 순서를 바꿔 명령을 실행할 수 있다.
- 싱글쓰레드에서는 관계가 없지만 멀티쓰레드에서는 관계가 있을 수 있다.

Write Buffering

문제는 메모리
- 읽고/쓰는 기계어가 순서대로 실행되지 않는다.
	. volatile로도 해결되지 않는다
	  - volatile 키워드는 기계어로 번역되지 않는다.
	. 읽기와 쓰기는 시간이 많이 걸리므로
	  - CPU의 입장에서 보면
	  - 실제 영향이 발휘되는 시간은 상대 Core에 따라 다르다.
	. 옆의 프로세서(Core)에서 

thread a		thread b
write x 1		write x 2
read x w		read x 1


- 메모리 변경 순서가 뒤바뀔 확률은?
- atomic_thread_fence()로 오류를 없애 보자
- atomic_int로 극복해 보자.

Cache Line Size Boundary
- 캐시를 읽을 때 캐시에 없으면 메모리에서 읽어 캐시에 넣는다.
- 이 때 한 바이트만 읽는 것이 아니라 64바이트 캐시 라인 전체를 읽어 넣는다.
- 그래야 메모리 낭비도 줄고 캐시 히트 확률도 올라간다.(공간적 지역성)
- DDR 메모리 특징은 한 메모리를 읽는 속도는 느리지만 그 다음 바이트를 읽는 속도는 매우 빠르다.

- 우리는 4바이트 메모리가 두 캐시 라인에 2바이트씩 걸치도록 만들었다.

- 중간값
	. Write시 최종 값과 초기값이 아닌 다른 값이 도중에 메모리에 써지는 현상
- 이유는?
	. Cache Line Size Boundary
- 대책
	. 포인터를 믿지 마라
	. Byte밖에 믿을 수 없다.
	. 포인터가 아닌 변수는 Visual C++가 잘 해준다.
  